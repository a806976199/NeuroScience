\section{Entropy and Mutual Information}
\label{sec:Entropy and Mutual Information}
In this chapter, the content of the first part is a brief introduction
to the basic knowledge of Shannon entropy and mutual information in
information theory. This part of the content is completely the
knowledge of information theory. The second part is about the problem
of maximizing the channel capacity of neurons, and discussed from two
aspects of single neuron encoding and group neuron encoding. The third
part is the application in the retinal ganglion receptive field. The
last part is about information analysis in the sense of temporal
coding (Spike Train).
There are two main concepts in this section. One is Shannon Entropy,
\begin{equation}
  \label{equ:4.3}
  H(X)=-\sum_{x}P[x]\log_2P[x].
\end{equation}
The other is Mutual Information,
\begin{equation}
\begin{aligned}
  I(X,Y)&=H(X)-H(X|Y)\\
  &=\sum_{x,y}P(x,y)\log_2\frac{P(x,y)}{P(x)P(y)}\\
&=KL(P(X,Y)\|P(X)P(Y)).
\end{aligned}
\end{equation}\\
Let the variable $x$ be the firing rate $r$, and let the variable $Y$
be the stimulus $s$, and we get the representation of information
theory related concepts in the neural channel. The representation in
the continuous sense is similar, except that a divergent term is
generated in the Shannon entropy. But due to the integral operation,
this term is eliminated in the subtraction operation of the mutual
information, so we often directly use
\begin{equation}
  \int p[r]\log_2p[r]dr,
\end{equation}
to represent Shannon entropy $H(r)$.\\

The intuitive understanding is that if the stimulus $s$ is not fixed,
the uncertainty $H(r)$ is large; when $s$ is fixed, the uncertainty
$H(r)$ is small, then the difference between the corresponding two
uncertainties is the information entropy. The larger the $I(r,s)$, we
can say that the $r$ can better reflect the changes or characteristics
of the $s$ (Responds are more informative about the identity of
stimulus.)\\

For mutual information, because of $I(X,Y)=I(Y,X)$, the status between
input information and output information is actually equivalent. The
ability to derive $s$ from $r$ is equal to the ability to derive $r$
from $s$.\\

In addition, there is a close connection between mutual information
and Fisher information (see Chapter 3). The mutual information
reflects the uncertainty of a local parameter $s$ distributed at the
peak, while the mutual information reflects the uncertainty of the
parameter $s$ in the global sense. When the number of neurons is
sufficiently large, we can directly assume that the neural activity
$P[\mathbf{r}|s]$ is Gaussian. As mentioned earlier, the Fisher
information of a Gaussian distribution at the mean is its variance
$\sigma^2$ multiplied by a constant, while the entropy of a Gaussian
distribution is
\begin{equation}
  H(N(\mu,\sigma^2))=\frac{1}{2}\ln 2\pi e\sigma^2,
\end{equation}
the two can be expressed relative to each other.\\

It is worth mentioning that the analysis in this section is carried out in the sense of rate code. The analysis between rate code and temporal code has been detailed in the first chapter. At that time, it was mentioned that some studies showed that the information carried by temporal code may be less than $10\%$ of the total information in most cases. In fact, it may also because we do not yet have a complete understanding of how information is encoded, and our failure to determine what is information and what is noise in neural activity. In any case, under the assumption of rate code, it is really convenient to analyze. Later in this chapter, the analysis will also be done from the perspective of the Interspike Interval Distribution (ISI).



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../notesOnFluidMechanics"
%%% End:
